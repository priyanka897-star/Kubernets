C:\Users\User>kubectl config unset clusters
Property "clusters" unset.

C:\Users\User>eksctl create cluster -f eks-cluster.yaml
2022-03-22 16:17:33 [ℹ]  eksctl version 0.86.0
2022-03-22 16:17:33 [ℹ]  using region us-east-2
2022-03-22 16:17:34 [ℹ]  setting availability zones to [us-east-2b us-east-2a us-east-2c]
2022-03-22 16:17:34 [ℹ]  subnets for us-east-2b - public:192.168.0.0/19 private:192.168.96.0/19
2022-03-22 16:17:34 [ℹ]  subnets for us-east-2a - public:192.168.32.0/19 private:192.168.128.0/19
2022-03-22 16:17:34 [ℹ]  subnets for us-east-2c - public:192.168.64.0/19 private:192.168.160.0/19
2022-03-22 16:17:35 [ℹ]  nodegroup "node-ing" will use "ami-0656dd273bd6e9a2f" [AmazonLinux2/1.21]
2022-03-22 16:17:35 [ℹ]  using Kubernetes version 1.21
2022-03-22 16:17:35 [ℹ]  creating EKS cluster "chatapp" in "us-east-2" region with un-managed nodes
2022-03-22 16:17:35 [ℹ]  1 nodegroup (node-ing) was included (based on the include/exclude rules)
2022-03-22 16:17:35 [ℹ]  will create a CloudFormation stack for cluster itself and 1 nodegroup stack(s)
2022-03-22 16:17:35 [ℹ]  will create a CloudFormation stack for cluster itself and 0 managed nodegroup stack(s)
2022-03-22 16:17:35 [ℹ]  if you encounter any issues, check CloudFormation console or try 'eksctl utils describe-stacks --region=us-east-2 --cluster=chatapp'
2022-03-22 16:17:35 [ℹ]  Kubernetes API endpoint access will use default of {publicAccess=true, privateAccess=false} for cluster "chatapp" in "us-east-2"
2022-03-22 16:17:35 [ℹ]  CloudWatch logging will not be enabled for cluster "chatapp" in "us-east-2"
2022-03-22 16:17:35 [ℹ]  you can enable it with 'eksctl utils update-cluster-logging --enable-types={SPECIFY-YOUR-LOG-TYPES-HERE (e.g. all)} --region=us-east-2 --cluster=chatapp'
2022-03-22 16:17:35 [ℹ]
2 sequential tasks: { create cluster control plane "chatapp",
    2 sequential sub-tasks: {
        wait for control plane to become ready,
        create nodegroup "node-ing",
    }
}
2022-03-22 16:17:35 [ℹ]  building cluster stack "eksctl-chatapp-cluster"
2022-03-22 16:17:37 [!]  1 error(s) occurred and cluster hasn't been created properly, you may wish to check CloudFormation console
2022-03-22 16:17:37 [ℹ]  to cleanup resources, run 'eksctl delete cluster --region=us-east-2 --name=chatapp'
2022-03-22 16:17:37 [✖]  creating CloudFormation stack "eksctl-chatapp-cluster": AlreadyExistsException: Stack [eksctl-chatapp-cluster] already exists
        status code: 400, request id: 5820a0e0-d359-4a12-a335-e513a6d51f91
Error: failed to create cluster "chatapp"

C:\Users\User>eksctl create cluster -f eks-cluster.yaml
2022-03-22 16:19:24 [ℹ]  eksctl version 0.86.0
2022-03-22 16:19:24 [ℹ]  using region us-east-2
2022-03-22 16:19:25 [ℹ]  setting availability zones to [us-east-2a us-east-2b us-east-2c]
2022-03-22 16:19:25 [ℹ]  subnets for us-east-2a - public:192.168.0.0/19 private:192.168.96.0/19
2022-03-22 16:19:25 [ℹ]  subnets for us-east-2b - public:192.168.32.0/19 private:192.168.128.0/19
2022-03-22 16:19:25 [ℹ]  subnets for us-east-2c - public:192.168.64.0/19 private:192.168.160.0/19
2022-03-22 16:19:26 [ℹ]  nodegroup "node-ing" will use "ami-0656dd273bd6e9a2f" [AmazonLinux2/1.21]
2022-03-22 16:19:26 [ℹ]  using Kubernetes version 1.21
2022-03-22 16:19:26 [ℹ]  creating EKS cluster "chatapp" in "us-east-2" region with un-managed nodes
2022-03-22 16:19:26 [ℹ]  1 nodegroup (node-ing) was included (based on the include/exclude rules)
2022-03-22 16:19:26 [ℹ]  will create a CloudFormation stack for cluster itself and 1 nodegroup stack(s)
2022-03-22 16:19:26 [ℹ]  will create a CloudFormation stack for cluster itself and 0 managed nodegroup stack(s)
2022-03-22 16:19:26 [ℹ]  if you encounter any issues, check CloudFormation console or try 'eksctl utils describe-stacks --region=us-east-2 --cluster=chatapp'
2022-03-22 16:19:26 [ℹ]  Kubernetes API endpoint access will use default of {publicAccess=true, privateAccess=false} for cluster "chatapp" in "us-east-2"
2022-03-22 16:19:26 [ℹ]  CloudWatch logging will not be enabled for cluster "chatapp" in "us-east-2"
2022-03-22 16:19:26 [ℹ]  you can enable it with 'eksctl utils update-cluster-logging --enable-types={SPECIFY-YOUR-LOG-TYPES-HERE (e.g. all)} --region=us-east-2 --cluster=chatapp'
2022-03-22 16:19:26 [ℹ]
2 sequential tasks: { create cluster control plane "chatapp",
    2 sequential sub-tasks: {
        wait for control plane to become ready,
        create nodegroup "node-ing",
    }
}
2022-03-22 16:19:26 [ℹ]  building cluster stack "eksctl-chatapp-cluster"
2022-03-22 16:19:27 [!]  1 error(s) occurred and cluster hasn't been created properly, you may wish to check CloudFormation console
2022-03-22 16:19:27 [ℹ]  to cleanup resources, run 'eksctl delete cluster --region=us-east-2 --name=chatapp'
2022-03-22 16:19:27 [✖]  creating CloudFormation stack "eksctl-chatapp-cluster": AlreadyExistsException: Stack [eksctl-chatapp-cluster] already exists
        status code: 400, request id: 3a447fda-92de-4ece-a82c-3b0910fe2722
Error: failed to create cluster "chatapp"

C:\Users\User>eksctl create cluster -f eks-cluster.yaml
2022-03-22 16:23:08 [ℹ]  eksctl version 0.86.0
2022-03-22 16:23:08 [ℹ]  using region us-east-2
2022-03-22 16:23:09 [ℹ]  setting availability zones to [us-east-2b us-east-2c us-east-2a]
2022-03-22 16:23:09 [ℹ]  subnets for us-east-2b - public:192.168.0.0/19 private:192.168.96.0/19
2022-03-22 16:23:09 [ℹ]  subnets for us-east-2c - public:192.168.32.0/19 private:192.168.128.0/19
2022-03-22 16:23:09 [ℹ]  subnets for us-east-2a - public:192.168.64.0/19 private:192.168.160.0/19
2022-03-22 16:23:10 [ℹ]  nodegroup "node-ing" will use "ami-0656dd273bd6e9a2f" [AmazonLinux2/1.21]
2022-03-22 16:23:10 [ℹ]  using Kubernetes version 1.21
2022-03-22 16:23:10 [ℹ]  creating EKS cluster "chatapp" in "us-east-2" region with un-managed nodes
2022-03-22 16:23:10 [ℹ]  1 nodegroup (node-ing) was included (based on the include/exclude rules)
2022-03-22 16:23:10 [ℹ]  will create a CloudFormation stack for cluster itself and 1 nodegroup stack(s)
2022-03-22 16:23:10 [ℹ]  will create a CloudFormation stack for cluster itself and 0 managed nodegroup stack(s)
2022-03-22 16:23:10 [ℹ]  if you encounter any issues, check CloudFormation console or try 'eksctl utils describe-stacks --region=us-east-2 --cluster=chatapp'
2022-03-22 16:23:10 [ℹ]  Kubernetes API endpoint access will use default of {publicAccess=true, privateAccess=false} for cluster "chatapp" in "us-east-2"
2022-03-22 16:23:10 [ℹ]  CloudWatch logging will not be enabled for cluster "chatapp" in "us-east-2"
2022-03-22 16:23:10 [ℹ]  you can enable it with 'eksctl utils update-cluster-logging --enable-types={SPECIFY-YOUR-LOG-TYPES-HERE (e.g. all)} --region=us-east-2 --cluster=chatapp'
2022-03-22 16:23:10 [ℹ]
2 sequential tasks: { create cluster control plane "chatapp",
    2 sequential sub-tasks: {
        wait for control plane to become ready,
        create nodegroup "node-ing",
    }
}
2022-03-22 16:23:10 [ℹ]  building cluster stack "eksctl-chatapp-cluster"
2022-03-22 16:23:12 [!]  1 error(s) occurred and cluster hasn't been created properly, you may wish to check CloudFormation console
2022-03-22 16:23:12 [ℹ]  to cleanup resources, run 'eksctl delete cluster --region=us-east-2 --name=chatapp'
2022-03-22 16:23:12 [✖]  creating CloudFormation stack "eksctl-chatapp-cluster": AlreadyExistsException: Stack [eksctl-chatapp-cluster] already exists
        status code: 400, request id: 0243b414-5178-4a14-8bb4-cb98bfffde24
Error: failed to create cluster "chatapp"

C:\Users\User>kubectl get cluster
Unable to connect to the server: dial tcp [::1]:8080: connectex: No connection could be made because the target machine actively refused it.

C:\Users\User>eksctl create cluster -f eks-cluster.yaml
2022-03-22 16:29:25 [ℹ]  eksctl version 0.86.0
2022-03-22 16:29:25 [ℹ]  using region us-east-2
2022-03-22 16:29:26 [ℹ]  setting availability zones to [us-east-2c us-east-2b us-east-2a]
2022-03-22 16:29:26 [ℹ]  subnets for us-east-2c - public:192.168.0.0/19 private:192.168.96.0/19
2022-03-22 16:29:26 [ℹ]  subnets for us-east-2b - public:192.168.32.0/19 private:192.168.128.0/19
2022-03-22 16:29:26 [ℹ]  subnets for us-east-2a - public:192.168.64.0/19 private:192.168.160.0/19
2022-03-22 16:29:27 [ℹ]  nodegroup "node-ing" will use "ami-0656dd273bd6e9a2f" [AmazonLinux2/1.21]
2022-03-22 16:29:28 [ℹ]  using Kubernetes version 1.21
2022-03-22 16:29:28 [ℹ]  creating EKS cluster "chatapp" in "us-east-2" region with un-managed nodes
2022-03-22 16:29:28 [ℹ]  1 nodegroup (node-ing) was included (based on the include/exclude rules)
2022-03-22 16:29:28 [ℹ]  will create a CloudFormation stack for cluster itself and 1 nodegroup stack(s)
2022-03-22 16:29:28 [ℹ]  will create a CloudFormation stack for cluster itself and 0 managed nodegroup stack(s)
2022-03-22 16:29:28 [ℹ]  if you encounter any issues, check CloudFormation console or try 'eksctl utils describe-stacks --region=us-east-2 --cluster=chatapp'
2022-03-22 16:29:28 [ℹ]  Kubernetes API endpoint access will use default of {publicAccess=true, privateAccess=false} for cluster "chatapp" in "us-east-2"
2022-03-22 16:29:28 [ℹ]  CloudWatch logging will not be enabled for cluster "chatapp" in "us-east-2"
2022-03-22 16:29:28 [ℹ]  you can enable it with 'eksctl utils update-cluster-logging --enable-types={SPECIFY-YOUR-LOG-TYPES-HERE (e.g. all)} --region=us-east-2 --cluster=chatapp'
2022-03-22 16:29:28 [ℹ]
2 sequential tasks: { create cluster control plane "chatapp",
    2 sequential sub-tasks: {
        wait for control plane to become ready,
        create nodegroup "node-ing",
    }
}
2022-03-22 16:29:28 [ℹ]  building cluster stack "eksctl-chatapp-cluster"
2022-03-22 16:29:29 [!]  1 error(s) occurred and cluster hasn't been created properly, you may wish to check CloudFormation console
2022-03-22 16:29:29 [ℹ]  to cleanup resources, run 'eksctl delete cluster --region=us-east-2 --name=chatapp'
2022-03-22 16:29:29 [✖]  creating CloudFormation stack "eksctl-chatapp-cluster": AlreadyExistsException: Stack [eksctl-chatapp-cluster] already exists
        status code: 400, request id: dfa7b845-d762-4521-9c64-a2c97134cde6
Error: failed to create cluster "chatapp"

C:\Users\User>eksctl create cluster -f eks-cluster.yaml
2022-03-22 16:31:46 [ℹ]  eksctl version 0.86.0
2022-03-22 16:31:46 [ℹ]  using region us-east-2
2022-03-22 16:31:47 [ℹ]  setting availability zones to [us-east-2c us-east-2b us-east-2a]
2022-03-22 16:31:47 [ℹ]  subnets for us-east-2c - public:192.168.0.0/19 private:192.168.96.0/19
2022-03-22 16:31:47 [ℹ]  subnets for us-east-2b - public:192.168.32.0/19 private:192.168.128.0/19
2022-03-22 16:31:47 [ℹ]  subnets for us-east-2a - public:192.168.64.0/19 private:192.168.160.0/19
2022-03-22 16:31:48 [ℹ]  nodegroup "node-ing" will use "ami-0656dd273bd6e9a2f" [AmazonLinux2/1.21]
2022-03-22 16:31:48 [ℹ]  using Kubernetes version 1.21
2022-03-22 16:31:48 [ℹ]  creating EKS cluster "chatapp" in "us-east-2" region with un-managed nodes
2022-03-22 16:31:48 [ℹ]  1 nodegroup (node-ing) was included (based on the include/exclude rules)
2022-03-22 16:31:48 [ℹ]  will create a CloudFormation stack for cluster itself and 1 nodegroup stack(s)
2022-03-22 16:31:48 [ℹ]  will create a CloudFormation stack for cluster itself and 0 managed nodegroup stack(s)
2022-03-22 16:31:48 [ℹ]  if you encounter any issues, check CloudFormation console or try 'eksctl utils describe-stacks --region=us-east-2 --cluster=chatapp'
2022-03-22 16:31:48 [ℹ]  Kubernetes API endpoint access will use default of {publicAccess=true, privateAccess=false} for cluster "chatapp" in "us-east-2"
2022-03-22 16:31:48 [ℹ]  CloudWatch logging will not be enabled for cluster "chatapp" in "us-east-2"
2022-03-22 16:31:48 [ℹ]  you can enable it with 'eksctl utils update-cluster-logging --enable-types={SPECIFY-YOUR-LOG-TYPES-HERE (e.g. all)} --region=us-east-2 --cluster=chatapp'
2022-03-22 16:31:48 [ℹ]
2 sequential tasks: { create cluster control plane "chatapp",
    2 sequential sub-tasks: {
        wait for control plane to become ready,
        create nodegroup "node-ing",
    }
}
2022-03-22 16:31:48 [ℹ]  building cluster stack "eksctl-chatapp-cluster"
2022-03-22 16:31:50 [ℹ]  deploying stack "eksctl-chatapp-cluster"
2022-03-22 16:32:20 [ℹ]  waiting for CloudFormation stack "eksctl-chatapp-cluster"
2022-03-22 16:32:51 [ℹ]  waiting for CloudFormation stack "eksctl-chatapp-cluster"
2022-03-22 16:33:52 [ℹ]  waiting for CloudFormation stack "eksctl-chatapp-cluster"
2022-03-22 16:34:53 [ℹ]  waiting for CloudFormation stack "eksctl-chatapp-cluster"
2022-03-22 16:35:54 [ℹ]  waiting for CloudFormation stack "eksctl-chatapp-cluster"
2022-03-22 16:36:55 [ℹ]  waiting for CloudFormation stack "eksctl-chatapp-cluster"
2022-03-22 16:37:56 [ℹ]  waiting for CloudFormation stack "eksctl-chatapp-cluster"
2022-03-22 16:38:57 [ℹ]  waiting for CloudFormation stack "eksctl-chatapp-cluster"
2022-03-22 16:39:58 [ℹ]  waiting for CloudFormation stack "eksctl-chatapp-cluster"
2022-03-22 16:40:59 [ℹ]  waiting for CloudFormation stack "eksctl-chatapp-cluster"
2022-03-22 16:42:00 [ℹ]  waiting for CloudFormation stack "eksctl-chatapp-cluster"
2022-03-22 16:43:01 [ℹ]  waiting for CloudFormation stack "eksctl-chatapp-cluster"
2022-03-22 16:45:08 [ℹ]  building nodegroup stack "eksctl-chatapp-nodegroup-node-ing"
2022-03-22 16:45:08 [ℹ]  --nodes-min=2 was set automatically for nodegroup node-ing
2022-03-22 16:45:08 [ℹ]  --nodes-max=2 was set automatically for nodegroup node-ing
2022-03-22 16:45:09 [ℹ]  deploying stack "eksctl-chatapp-nodegroup-node-ing"
2022-03-22 16:45:09 [ℹ]  waiting for CloudFormation stack "eksctl-chatapp-nodegroup-node-ing"
2022-03-22 16:45:28 [ℹ]  waiting for CloudFormation stack "eksctl-chatapp-nodegroup-node-ing"
2022-03-22 16:45:49 [ℹ]  waiting for CloudFormation stack "eksctl-chatapp-nodegroup-node-ing"
2022-03-22 16:46:09 [ℹ]  waiting for CloudFormation stack "eksctl-chatapp-nodegroup-node-ing"
2022-03-22 16:46:28 [ℹ]  waiting for CloudFormation stack "eksctl-chatapp-nodegroup-node-ing"
2022-03-22 17:55:29 [✖]  unexpected status "CREATE_COMPLETE" while waiting for CloudFormation stack "eksctl-chatapp-nodegroup-node-ing"
2022-03-22 17:55:29 [ℹ]  fetching stack events in attempt to troubleshoot the root cause of the failure
2022-03-22 17:55:30 [!]  1 error(s) occurred and cluster hasn't been created properly, you may wish to check CloudFormation console
2022-03-22 17:55:30 [ℹ]  to cleanup resources, run 'eksctl delete cluster --region=us-east-2 --name=chatapp'
2022-03-22 17:55:30 [✖]  waiting for CloudFormation stack "eksctl-chatapp-nodegroup-node-ing": RequestCanceled: waiter context canceled
caused by: context deadline exceeded
Error: failed to create cluster "chatapp"

C:\Users\User>aws configure
AWS Access Key ID 
AWS Secret Access Key [****************DJnM]: 
Default region name [us-east-2]:
Default output format [json]:

C:\Users\User>eksctl create cluster -f mycluster.yaml
2022-03-22 18:16:41 [ℹ]  eksctl version 0.86.0
2022-03-22 18:16:41 [ℹ]  using region us-east-2
2022-03-22 18:16:42 [ℹ]  setting availability zones to [us-east-2c us-east-2b us-east-2a]
2022-03-22 18:16:42 [ℹ]  subnets for us-east-2c - public:192.168.0.0/19 private:192.168.96.0/19
2022-03-22 18:16:42 [ℹ]  subnets for us-east-2b - public:192.168.32.0/19 private:192.168.128.0/19
2022-03-22 18:16:42 [ℹ]  subnets for us-east-2a - public:192.168.64.0/19 private:192.168.160.0/19
2022-03-22 18:16:43 [ℹ]  nodegroup "node-ing" will use "ami-0656dd273bd6e9a2f" [AmazonLinux2/1.21]
2022-03-22 18:16:43 [ℹ]  using Kubernetes version 1.21
2022-03-22 18:16:43 [ℹ]  creating EKS cluster "chatapp" in "us-east-2" region with un-managed nodes
2022-03-22 18:16:43 [ℹ]  1 nodegroup (node-ing) was included (based on the include/exclude rules)
2022-03-22 18:16:43 [ℹ]  will create a CloudFormation stack for cluster itself and 1 nodegroup stack(s)
2022-03-22 18:16:43 [ℹ]  will create a CloudFormation stack for cluster itself and 0 managed nodegroup stack(s)
2022-03-22 18:16:43 [ℹ]  if you encounter any issues, check CloudFormation console or try 'eksctl utils describe-stacks --region=us-east-2 --cluster=chatapp'
2022-03-22 18:16:43 [ℹ]  Kubernetes API endpoint access will use default of {publicAccess=true, privateAccess=false} for cluster "chatapp" in "us-east-2"
2022-03-22 18:16:43 [ℹ]  CloudWatch logging will not be enabled for cluster "chatapp" in "us-east-2"
2022-03-22 18:16:44 [ℹ]  you can enable it with 'eksctl utils update-cluster-logging --enable-types={SPECIFY-YOUR-LOG-TYPES-HERE (e.g. all)} --region=us-east-2 --cluster=chatapp'
2022-03-22 18:16:44 [ℹ]
2 sequential tasks: { create cluster control plane "chatapp",
    2 sequential sub-tasks: {
        wait for control plane to become ready,
        create nodegroup "node-ing",
    }
}
2022-03-22 18:16:44 [ℹ]  building cluster stack "eksctl-chatapp-cluster"
2022-03-22 18:16:50 [ℹ]  deploying stack "eksctl-chatapp-cluster"
2022-03-22 18:17:20 [ℹ]  waiting for CloudFormation stack "eksctl-chatapp-cluster"
2022-03-22 18:17:52 [ℹ]  waiting for CloudFormation stack "eksctl-chatapp-cluster"
2022-03-22 18:18:52 [ℹ]  waiting for CloudFormation stack "eksctl-chatapp-cluster"
2022-03-22 18:19:54 [ℹ]  waiting for CloudFormation stack "eksctl-chatapp-cluster"
2022-03-22 18:20:55 [ℹ]  waiting for CloudFormation stack "eksctl-chatapp-cluster"
2022-03-22 18:21:57 [ℹ]  waiting for CloudFormation stack "eksctl-chatapp-cluster"
2022-03-22 18:22:59 [ℹ]  waiting for CloudFormation stack "eksctl-chatapp-cluster"
2022-03-22 18:24:00 [ℹ]  waiting for CloudFormation stack "eksctl-chatapp-cluster"
2022-03-22 18:25:01 [ℹ]  waiting for CloudFormation stack "eksctl-chatapp-cluster"
2022-03-22 18:26:02 [ℹ]  waiting for CloudFormation stack "eksctl-chatapp-cluster"
2022-03-22 18:27:03 [ℹ]  waiting for CloudFormation stack "eksctl-chatapp-cluster"
2022-03-22 18:29:12 [ℹ]  building nodegroup stack "eksctl-chatapp-nodegroup-node-ing"
2022-03-22 18:29:12 [ℹ]  --nodes-min=2 was set automatically for nodegroup node-ing
2022-03-22 18:29:12 [ℹ]  --nodes-max=2 was set automatically for nodegroup node-ing
2022-03-22 18:29:15 [ℹ]  deploying stack "eksctl-chatapp-nodegroup-node-ing"
2022-03-22 18:29:15 [ℹ]  waiting for CloudFormation stack "eksctl-chatapp-nodegroup-node-ing"
2022-03-22 18:29:34 [ℹ]  waiting for CloudFormation stack "eksctl-chatapp-nodegroup-node-ing"
2022-03-22 18:29:55 [ℹ]  waiting for CloudFormation stack "eksctl-chatapp-nodegroup-node-ing"
2022-03-22 18:30:16 [ℹ]  waiting for CloudFormation stack "eksctl-chatapp-nodegroup-node-ing"
2022-03-22 18:30:32 [ℹ]  waiting for CloudFormation stack "eksctl-chatapp-nodegroup-node-ing"
2022-03-22 18:30:52 [ℹ]  waiting for CloudFormation stack "eksctl-chatapp-nodegroup-node-ing"
2022-03-22 18:31:09 [ℹ]  waiting for CloudFormation stack "eksctl-chatapp-nodegroup-node-ing"
2022-03-22 18:31:26 [ℹ]  waiting for CloudFormation stack "eksctl-chatapp-nodegroup-node-ing"
2022-03-22 18:31:44 [ℹ]  waiting for CloudFormation stack "eksctl-chatapp-nodegroup-node-ing"
2022-03-22 18:32:01 [ℹ]  waiting for CloudFormation stack "eksctl-chatapp-nodegroup-node-ing"
2022-03-22 18:32:17 [ℹ]  waiting for CloudFormation stack "eksctl-chatapp-nodegroup-node-ing"
2022-03-22 18:32:34 [ℹ]  waiting for CloudFormation stack "eksctl-chatapp-nodegroup-node-ing"
2022-03-22 18:32:51 [ℹ]  waiting for CloudFormation stack "eksctl-chatapp-nodegroup-node-ing"
2022-03-22 18:32:52 [ℹ]  waiting for the control plane availability...
2022-03-22 18:32:53 [✔]  saved kubeconfig as "C:\\Users\\User\\.kube\\config"
2022-03-22 18:32:53 [ℹ]  no tasks
2022-03-22 18:32:53 [✔]  all EKS cluster resources for "chatapp" have been created
2022-03-22 18:32:55 [ℹ]  adding identity "arn:aws:iam::285938675002:role/eksctl-chatapp-nodegroup-node-ing-NodeInstanceRole-7802R8RU5ZWH" to auth ConfigMap
2022-03-22 18:32:55 [ℹ]  nodegroup "node-ing" has 0 node(s)
2022-03-22 18:32:55 [ℹ]  waiting for at least 2 node(s) to become ready in "node-ing"
2022-03-22 18:33:31 [ℹ]  nodegroup "node-ing" has 2 node(s)
2022-03-22 18:33:31 [ℹ]  node "ip-192-168-25-187.us-east-2.compute.internal" is ready
2022-03-22 18:33:31 [ℹ]  node "ip-192-168-51-66.us-east-2.compute.internal" is ready
2022-03-22 18:34:15 [ℹ]  kubectl command should work with "C:\\Users\\User\\.kube\\config", try 'kubectl get nodes'
2022-03-22 18:34:15 [✔]  EKS cluster "chatapp" in "us-east-2" region is ready

C:\Users\User>
C:\Users\User>
C:\Users\User>
C:\Users\User>
C:\Users\User>
C:\Users\User>
C:\Users\User>eksctl utils describe-stacks --region=us-east-2 --cluster=chatapp
2022-03-22 18:36:17 [ℹ]  eksctl version 0.86.0
2022-03-22 18:36:18 [ℹ]  using region us-east-2
2022-03-22 18:36:22 [ℹ]  stack/eksctl-chatapp-nodegroup-node-ing = {
  Capabilities: ["CAPABILITY_IAM"],
  CreationTime: 2022-03-22 12:59:15.984 +0000 UTC,
  Description: "EKS nodes (AMI family: AmazonLinux2, SSH access: false, private networking: false) [created and managed by eksctl]",
  DisableRollback: false,
  DriftInformation: {
    StackDriftStatus: "NOT_CHECKED"
  },
  EnableTerminationProtection: false,
  Outputs: [
    {
      OutputKey: "FeaturePrivateNetworking",
      OutputValue: "false"
    },
    {
      ExportName: "eksctl-chatapp-nodegroup-node-ing::InstanceRoleARN",
      OutputKey: "InstanceRoleARN",
      OutputValue: "arn:aws:iam::285938675002:role/eksctl-chatapp-nodegroup-node-ing-NodeInstanceRole-7802R8RU5ZWH"
    },
    {
      OutputKey: "FeatureLocalSecurityGroup",
      OutputValue: "true"
    },
    {
      ExportName: "eksctl-chatapp-nodegroup-node-ing::InstanceProfileARN",
      OutputKey: "InstanceProfileARN",
      OutputValue: "arn:aws:iam::285938675002:instance-profile/eksctl-chatapp-nodegroup-node-ing-NodeInstanceProfile-1AAJBX0LEHWJA"
    },
    {
      OutputKey: "FeatureSharedSecurityGroup",
      OutputValue: "true"
    }
  ],
  RollbackConfiguration: {

  },
  StackId: "arn:aws:cloudformation:us-east-2:285938675002:stack/eksctl-chatapp-nodegroup-node-ing/e10030d0-a9df-11ec-84d4-0aa5a4c8e264",
  StackName: "eksctl-chatapp-nodegroup-node-ing",
  StackStatus: "CREATE_COMPLETE",
  Tags: [
    {
      Key: "alpha.eksctl.io/cluster-name",
      Value: "chatapp"
    },
    {
      Key: "alpha.eksctl.io/nodegroup-name",
      Value: "node-ing"
    },
    {
      Key: "eksctl.cluster.k8s.io/v1alpha1/cluster-name",
      Value: "chatapp"
    },
    {
      Key: "alpha.eksctl.io/nodegroup-type",
      Value: "unmanaged"
    },
    {
      Key: "alpha.eksctl.io/eksctl-version",
      Value: "0.86.0"
    },
    {
      Key: "eksctl.io/v1alpha2/nodegroup-name",
      Value: "node-ing"
    }
  ]
}
2022-03-22 18:36:22 [ℹ]  stack/eksctl-chatapp-cluster = {
  Capabilities: ["CAPABILITY_IAM"],
  CreationTime: 2022-03-22 12:46:51.37 +0000 UTC,
  Description: "EKS cluster (dedicated VPC: true, dedicated IAM: true) [created and managed by eksctl]",
  DisableRollback: false,
  DriftInformation: {
    StackDriftStatus: "NOT_CHECKED"
  },
  EnableTerminationProtection: false,
  Outputs: [
    {
      ExportName: "eksctl-chatapp-cluster::SubnetsPrivate",
      OutputKey: "SubnetsPrivate",
      OutputValue: "subnet-0cd554b8d7938614d,subnet-09feee1d040e17b7d,subnet-0194067792dc0fc7a"
    },
    {
      ExportName: "eksctl-chatapp-cluster::SubnetsPublic",
      OutputKey: "SubnetsPublic",
      OutputValue: "subnet-00a182b7474ac6618,subnet-059f7fb460c330dbd,subnet-0a296f8521f5ae55c"
    },
    {
      OutputKey: "FeatureNATMode",
      OutputValue: "Single"
    },
    {
      ExportName: "eksctl-chatapp-cluster::ServiceRoleARN",
      OutputKey: "ServiceRoleARN",
      OutputValue: "arn:aws:iam::285938675002:role/eksctl-chatapp-cluster-ServiceRole-1TBNNK4ZPYTMI"
    },
    {
      ExportName: "eksctl-chatapp-cluster::Endpoint",
      OutputKey: "Endpoint",
      OutputValue: "https://B11FE0E446521E8B74FAE139608D8C6A.gr7.us-east-2.eks.amazonaws.com"
    },
    {
      ExportName: "eksctl-chatapp-cluster::SharedNodeSecurityGroup",
      OutputKey: "SharedNodeSecurityGroup",
      OutputValue: "sg-0a2570139841d8572"
    },
    {
      ExportName: "eksctl-chatapp-cluster::VPC",
      OutputKey: "VPC",
      OutputValue: "vpc-01e7925b31d808cb0"
    },
    {
      ExportName: "eksctl-chatapp-cluster::ClusterSecurityGroupId",
      OutputKey: "ClusterSecurityGroupId",
      OutputValue: "sg-0036fbb0148c1dc92"
    },
    {
      OutputKey: "ClusterStackName",
      OutputValue: "eksctl-chatapp-cluster"
    },
    {
      OutputKey: "CertificateAuthorityData",
      OutputValue: "LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUM1ekNDQWMrZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRJeU1ETXlNakV5TlRJME9Wb1hEVE15TURNeE9URXlOVEkwT1Zvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBTDh4CnVCQUtZb3R4ZkRSKzB4WXRwempzbDhvKzVnSkFQeExSakh4WVhKZHJSTlR4d29EZGJKeXpGWXRrSElaUUxhMzIKSDlhUnY0ZFdaNXdqSTNLZGNRZjZKUURYWGxmWmVrUEtsK3RTSmE5WkxwNE02eWRHam5qbmFINWVtZHB1dmtxVQpwMVl1VE9GbzBXT3dSK2daUDQzQW5ucVJuRlliNllSMjN1cTIxVlRwL3ZGdFlkTFgrbCtwRC9ZeVdObVJoV3RJCkNHSkVCWlprQzE1MW5iN1ZjK2hOR2RKNnJYclduQ1d0VWpwOWYyVW1FUXRoa2w4anc3S3o4clBZVTRISjhudHkKOU0zMXVUMnY2YWRQdnlMaEJqTkppdnhaczZmNWs3bEdEcXNPN3BqVG55V0FqRVJmVEdwZVVCMEFReDJKUU85Swo0YVd5ekVTNVFjYXVKMEluNWlrQ0F3RUFBYU5DTUVBd0RnWURWUjBQQVFIL0JBUURBZ0trTUE4R0ExVWRFd0VCCi93UUZNQU1CQWY4d0hRWURWUjBPQkJZRUZMcnloNDVUK3NmQk1ieUFVSlFBMytndk56QmdNQTBHQ1NxR1NJYjMKRFFFQkN3VUFBNElCQVFBSkcwb2VDOWR6a1dTUHkwaHF0T3EyNVBLRzU4anIyNG1XR1JhSGlGc3VLTE5QSXdLMgpDTkR4VjF4QXVwSXVKelVXZVkzZXZzNW9aM2NrRTlhbWNqeGRla0dRNC8zd3VVSjFHaVE5V1huUmhLZjU1OG5uCkprT3lBWjQ3SHg0SWFiWW9tVEl5NWhFMFRzL3JNeno0alZBS3JwZWdWRmNzbzYyRlV1blVDK3RqNXVzVWMyT0MKS0xwZkZRaXZUNFhrVlFFNE9HbDZ6TXI0dFRBcUx1Z0czRGViWm1MM2o1OUlsRXl5UkdCa0Y0TzM3eExLNTY3YwpMeTJ0U25oeTZDL1dvZXBmNzlkTUQrNm9TVjVGVjUvV1pKeUtoWDl1aThIdUl6a3dwc09LQktXTEdoek14WkhnCkJKQi96WVA0NDJkWTJKU2swQkF4Y0g3SC9zcEtLdGplUFpMOAotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg=="
    },
    {
      ExportName: "eksctl-chatapp-cluster::SecurityGroup",
      OutputKey: "SecurityGroup",
      OutputValue: "sg-0a0391922037b20e0"
    },
    {
      ExportName: "eksctl-chatapp-cluster::ARN",
      OutputKey: "ARN",
      OutputValue: "arn:aws:eks:us-east-2:285938675002:cluster/chatapp"
    }
  ],
  RollbackConfiguration: {

  },
  StackId: "arn:aws:cloudformation:us-east-2:285938675002:stack/eksctl-chatapp-cluster/252be3f0-a9de-11ec-bcab-0245d9ecd768",
  StackName: "eksctl-chatapp-cluster",
  StackStatus: "CREATE_COMPLETE",
  Tags: [{
      Key: "alpha.eksctl.io/cluster-name",
      Value: "chatapp"
    },{
      Key: "eksctl.cluster.k8s.io/v1alpha1/cluster-name",
      Value: "chatapp"
    },{
      Key: "alpha.eksctl.io/eksctl-version",
      Value: "0.86.0"
    }]
}

C:\Users\User>aws sts get-caller-identity
{
    "UserId": "AIDAUFE2F5U5EEYGQEMPA",
    "Account": "285938675002",
    "Arn": "arn:aws:iam::285938675002:user/priyanka"
}


C:\Users\User>eksctl utils write-kubeconfig --cluster=chatapp --set-kubeconfig-context=true
2022-03-22 18:37:27 [ℹ]  eksctl version 0.86.0
2022-03-22 18:37:27 [ℹ]  using region us-east-2
2022-03-22 18:37:29 [✔]  saved kubeconfig as "C:\\Users\\User\\.kube\\config"

C:\Users\User>eksctl utils associate-iam-oidc-provider --cluster=chatapp --approve
2022-03-22 18:38:06 [ℹ]  eksctl version 0.86.0
2022-03-22 18:38:06 [ℹ]  using region us-east-2
2022-03-22 18:38:09 [ℹ]  will create IAM Open ID Connect provider for cluster "chatapp" in "us-east-2"
2022-03-22 18:38:10 [✔]  created IAM Open ID Connect provider for cluster "chatapp" in "us-east-2"

C:\Users\User>kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/v1.1.4/docs/examples/rbac-role.yaml
clusterrole.rbac.authorization.k8s.io/alb-ingress-controller created
clusterrolebinding.rbac.authorization.k8s.io/alb-ingress-controller created
serviceaccount/alb-ingress-controller created

C:\Users\User>curl -o iam_policy.json https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.2.0/docs/install/iam_policy.json
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100  7273  100  7273    0     0   7273      0  0:00:01 --:--:--  0:00:01 12941

C:\Users\User>aws iam create-policy --policy-name AWSLoadBalancerControllerIAMPolicy --policy-document file://iam_policy.json

An error occurred (EntityAlreadyExists) when calling the CreatePolicy operation: A policy called AWSLoadBalancerControllerIAMPolicy already exists. Duplicate names are not allowed.

C:\Users\User>aws iam create-policy --policy-name AWSLoadBalancerControllerIAMPolicy --policy-document file://iam_policy.json
{
    "Policy": {
        "PolicyName": "AWSLoadBalancerControllerIAMPolicy",
        "PolicyId": "ANPAUFE2F5U5L6YMJOCO3",
        "Arn": "arn:aws:iam::285938675002:policy/AWSLoadBalancerControllerIAMPolicy",
        "Path": "/",
        "DefaultVersionId": "v1",
        "AttachmentCount": 0,
        "PermissionsBoundaryUsageCount": 0,
        "IsAttachable": true,
        "CreateDate": "2022-03-22T13:11:41+00:00",
        "UpdateDate": "2022-03-22T13:11:41+00:00"
    }
}


C:\Users\User>eksctl create iamserviceaccount --cluster=chatapp-cluster --namespace=kube-system --name=aws-load-balancer-controller --attach-policy-arn=arn:aws:iam::285938675002:policy/AWSLoadBalancerControllerIAMPolicy --override-existing-serviceaccounts --approve
2022-03-22 18:43:49 [ℹ]  eksctl version 0.86.0
2022-03-22 18:43:49 [ℹ]  using region us-east-2
Error: unable to describe cluster control plane: ResourceNotFoundException: No cluster found for name: chatapp-cluster.
{
  RespMetadata: {
    StatusCode: 404,
    RequestID: "a1774c2a-7589-4b7e-ae9d-ae3da7c09232"
  },
  Message_: "No cluster found for name: chatapp-cluster."
}

C:\Users\User>eksctl create iamserviceaccount --cluster=chatapp --namespace=kube-system --name=aws-load-balancer-controller --attach-policy-arn=arn:aws:iam::285938675002:policy/AWSLoadBalancerControllerIAMPolicy --override-existing-serviceaccounts --approve
2022-03-22 18:44:23 [ℹ]  eksctl version 0.86.0
2022-03-22 18:44:23 [ℹ]  using region us-east-2
2022-03-22 18:44:28 [ℹ]  1 iamserviceaccount (kube-system/aws-load-balancer-controller) was included (based on the include/exclude rules)
2022-03-22 18:44:28 [!]  metadata of serviceaccounts that exist in Kubernetes will be updated, as --override-existing-serviceaccounts was set
2022-03-22 18:44:28 [ℹ]  1 task: {
    2 sequential sub-tasks: {
        create IAM role for serviceaccount "kube-system/aws-load-balancer-controller",
        create serviceaccount "kube-system/aws-load-balancer-controller",
    } }2022-03-22 18:44:28 [ℹ]  building iamserviceaccount stack "eksctl-chatapp-addon-iamserviceaccount-kube-system-aws-load-balancer-controller"
2022-03-22 18:44:32 [ℹ]  deploying stack "eksctl-chatapp-addon-iamserviceaccount-kube-system-aws-load-balancer-controller"
2022-03-22 18:44:32 [ℹ]  waiting for CloudFormation stack "eksctl-chatapp-addon-iamserviceaccount-kube-system-aws-load-balancer-controller"
2022-03-22 18:44:49 [ℹ]  waiting for CloudFormation stack "eksctl-chatapp-addon-iamserviceaccount-kube-system-aws-load-balancer-controller"
2022-03-22 18:44:56 [ℹ]  created serviceaccount "kube-system/aws-load-balancer-controller"

C:\Users\User>kubectl apply --validate=false -f https://github.com/jetstack/cert-manager/releases/download/v1.1.1/cert-manager.yaml
customresourcedefinition.apiextensions.k8s.io/certificaterequests.cert-manager.io created
customresourcedefinition.apiextensions.k8s.io/certificates.cert-manager.io created
customresourcedefinition.apiextensions.k8s.io/challenges.acme.cert-manager.io created
customresourcedefinition.apiextensions.k8s.io/clusterissuers.cert-manager.io created
customresourcedefinition.apiextensions.k8s.io/issuers.cert-manager.io created
customresourcedefinition.apiextensions.k8s.io/orders.acme.cert-manager.io created
namespace/cert-manager created
serviceaccount/cert-manager-cainjector created
serviceaccount/cert-manager created
serviceaccount/cert-manager-webhook created
clusterrole.rbac.authorization.k8s.io/cert-manager-cainjector created
clusterrole.rbac.authorization.k8s.io/cert-manager-controller-issuers created
clusterrole.rbac.authorization.k8s.io/cert-manager-controller-clusterissuers created
clusterrole.rbac.authorization.k8s.io/cert-manager-controller-certificates created
clusterrole.rbac.authorization.k8s.io/cert-manager-controller-orders created
clusterrole.rbac.authorization.k8s.io/cert-manager-controller-challenges created
clusterrole.rbac.authorization.k8s.io/cert-manager-controller-ingress-shim created
clusterrole.rbac.authorization.k8s.io/cert-manager-view created
clusterrole.rbac.authorization.k8s.io/cert-manager-edit created
clusterrolebinding.rbac.authorization.k8s.io/cert-manager-cainjector created
clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-issuers created
clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-clusterissuers created
clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-certificates created
clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-orders created
clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-challenges created
clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-ingress-shim created
role.rbac.authorization.k8s.io/cert-manager-cainjector:leaderelection created
role.rbac.authorization.k8s.io/cert-manager:leaderelection created
role.rbac.authorization.k8s.io/cert-manager-webhook:dynamic-serving created
rolebinding.rbac.authorization.k8s.io/cert-manager-cainjector:leaderelection created
rolebinding.rbac.authorization.k8s.io/cert-manager:leaderelection created
rolebinding.rbac.authorization.k8s.io/cert-manager-webhook:dynamic-serving created
service/cert-manager created
service/cert-manager-webhook created
deployment.apps/cert-manager-cainjector created
deployment.apps/cert-manager created
deployment.apps/cert-manager-webhook created
mutatingwebhookconfiguration.admissionregistration.k8s.io/cert-manager-webhook created
validatingwebhookconfiguration.admissionregistration.k8s.io/cert-manager-webhook created

C:\Users\User>kubectl apply -k "github.com/aws/eks-charts/stable/aws-load-balancer-controller//crds?ref=master"
customresourcedefinition.apiextensions.k8s.io/ingressclassparams.elbv2.k8s.aws created
customresourcedefinition.apiextensions.k8s.io/targetgroupbindings.elbv2.k8s.aws created

C:\Users\User>curl -Lo v2_4_1_full.yaml https://github.com/kubernetes-sigs/aws-load-balancer-controller/releases/download/v2.4.1/v2_4_1_full.yaml
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100   653  100   653    0     0    653      0  0:00:01  0:00:01 --:--:--   557
100 34368  100 34368    0     0  17184      0  0:00:02  0:00:02 --:--:-- 21359

C:\Users\User>kubectl apply -f v2_4_1_full.yaml
customresourcedefinition.apiextensions.k8s.io/ingressclassparams.elbv2.k8s.aws configured
customresourcedefinition.apiextensions.k8s.io/targetgroupbindings.elbv2.k8s.aws configured
Warning: resource serviceaccounts/aws-load-balancer-controller is missing the kubectl.kubernetes.io/last-applied-configuration annotation which is required by kubectl apply. kubectl apply should only be used on resources created declaratively by either kubectl create --save-config or kubectl apply. The missing annotation will be patched automatically.
serviceaccount/aws-load-balancer-controller configured
role.rbac.authorization.k8s.io/aws-load-balancer-controller-leader-election-role created
clusterrole.rbac.authorization.k8s.io/aws-load-balancer-controller-role created
rolebinding.rbac.authorization.k8s.io/aws-load-balancer-controller-leader-election-rolebinding created
clusterrolebinding.rbac.authorization.k8s.io/aws-load-balancer-controller-rolebinding created
service/aws-load-balancer-webhook-service created
deployment.apps/aws-load-balancer-controller created
certificate.cert-manager.io/aws-load-balancer-serving-cert created
issuer.cert-manager.io/aws-load-balancer-selfsigned-issuer created
mutatingwebhookconfiguration.admissionregistration.k8s.io/aws-load-balancer-webhook created
validatingwebhookconfiguration.admissionregistration.k8s.io/aws-load-balancer-webhook created
ingressclassparams.elbv2.k8s.aws/alb created
ingressclass.networking.k8s.io/alb created

C:\Users\User>kubectl create namespace application
namespace/application created

C:\Users\User>kubectl apply -f db-secret.yaml -f db-deployment.yaml -f db-service.yaml -n application
secret/db-secret created
deployment.apps/db created
service/db-service created

C:\Users\User>kubectl apply -f app-configmap.yaml -f backendsecret.yaml -f chatappbackend.yaml -f backendservice.yaml -n application
configmap/app-configmap created
secret/app-secret created
deployment.apps/app1 created
service/backendapp created

C:\Users\User>kubectl get pods -n application
NAME                    READY   STATUS              RESTARTS   AGE
app1-6b664dc6cb-458tk   0/1     ContainerCreating   0          22s
app1-6b664dc6cb-rpx7x   0/1     ContainerCreating   0          22s
db-5696c95474-rslmv     1/1     Running             0          65s

C:\Users\User>kubectl get svc -n application
NAME         TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE
backendapp   NodePort    10.100.251.73   <none>        8000:31679/TCP   34s
db-service   ClusterIP   10.100.200.8    <none>        3306/TCP         78s

C:\Users\User>kubectl apply -f ingress-file.yaml
ingress.networking.k8s.io/app-ingress created

C:\Users\User>kubectl get all -n kube-system
NAME                                                READY   STATUS    RESTARTS   AGE
pod/aws-load-balancer-controller-8558654d4d-zrg6j   1/1     Running   0          5m56s
pod/aws-node-6pf2t                                  1/1     Running   0          24m
pod/aws-node-zn8cf                                  1/1     Running   0          24m
pod/coredns-f47955f89-69n7v                         1/1     Running   0          33m
pod/coredns-f47955f89-n2xbs                         1/1     Running   0          33m
pod/kube-proxy-cm9rm                                1/1     Running   0          24m
pod/kube-proxy-xxwpm                                1/1     Running   0          24m

NAME                                        TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)         AGE
service/aws-load-balancer-webhook-service   ClusterIP   10.100.240.141   <none>        443/TCP         5m59s
service/kube-dns                            ClusterIP   10.100.0.10      <none>        53/UDP,53/TCP   33m

NAME                        DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE
daemonset.apps/aws-node     2         2         2       2            2           <none>          33m
daemonset.apps/kube-proxy   2         2         2       2            2           <none>          33m

NAME                                           READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/aws-load-balancer-controller   1/1     1            1           6m
deployment.apps/coredns                        2/2     2            2           33m

NAME                                                      DESIRED   CURRENT   READY   AGE
replicaset.apps/aws-load-balancer-controller-8558654d4d   1         1         1       6m1s
replicaset.apps/coredns-f47955f89                         2         2         2       33m

C:\Users\User>kubectl logs pod/aws-load-balancer-controller-8558654d4d-zrg6j -n kube-system
{"level":"info","ts":1647955288.5603783,"msg":"version","GitVersion":"v2.4.1","GitCommit":"c4471defda10f8184173192b9f9df4e411ee5dac","BuildDate":"2022-03-16T16:10:01+0000"}
{"level":"info","ts":1647955288.6672072,"logger":"controller-runtime.metrics","msg":"metrics server is starting to listen","addr":":8080"}
{"level":"info","ts":1647955288.7486017,"logger":"setup","msg":"adding health check for controller"}
{"level":"info","ts":1647955288.7491958,"logger":"controller-runtime.webhook","msg":"registering webhook","path":"/mutate-v1-pod"}
{"level":"info","ts":1647955288.749286,"logger":"controller-runtime.webhook","msg":"registering webhook","path":"/mutate-elbv2-k8s-aws-v1beta1-targetgroupbinding"}
{"level":"info","ts":1647955288.7493513,"logger":"controller-runtime.webhook","msg":"registering webhook","path":"/validate-elbv2-k8s-aws-v1beta1-targetgroupbinding"}
{"level":"info","ts":1647955288.7494078,"logger":"controller-runtime.webhook","msg":"registering webhook","path":"/validate-networking-v1-ingress"}
{"level":"info","ts":1647955288.7494545,"logger":"setup","msg":"starting podInfo repo"}
{"level":"info","ts":1647955290.7498786,"logger":"controller-runtime.manager","msg":"starting metrics server","path":"/metrics"}
{"level":"info","ts":1647955290.7500327,"logger":"controller-runtime.webhook.webhooks","msg":"starting webhook server"}
{"level":"info","ts":1647955290.7503061,"logger":"controller-runtime.certwatcher","msg":"Updated current TLS certificate"}
{"level":"info","ts":1647955290.7503662,"logger":"controller-runtime.webhook","msg":"serving webhook server","host":"","port":9443}
{"level":"info","ts":1647955290.7508388,"logger":"controller-runtime.certwatcher","msg":"Starting certificate watcher"}
I0322 13:21:30.749688       1 leaderelection.go:243] attempting to acquire leader lease kube-system/aws-load-balancer-controller-leader...
I0322 13:21:30.773191       1 leaderelection.go:253] successfully acquired lease kube-system/aws-load-balancer-controller-leader
{"level":"info","ts":1647955290.8507805,"logger":"controller-runtime.manager.controller.targetGroupBinding","msg":"Starting EventSource","reconciler group":"elbv2.k8s.aws","reconciler kind":"TargetGroupBinding","source":"kind source: /, Kind="}
{"level":"info","ts":1647955290.850835,"logger":"controller-runtime.manager.controller.targetGroupBinding","msg":"Starting EventSource","reconciler group":"elbv2.k8s.aws","reconciler kind":"TargetGroupBinding","source":"kind source: /, Kind="}
{"level":"info","ts":1647955290.8508434,"logger":"controller-runtime.manager.controller.targetGroupBinding","msg":"Starting EventSource","reconciler group":"elbv2.k8s.aws","reconciler kind":"TargetGroupBinding","source":"kind source: /, Kind="}
{"level":"info","ts":1647955290.8508499,"logger":"controller-runtime.manager.controller.targetGroupBinding","msg":"Starting EventSource","reconciler group":"elbv2.k8s.aws","reconciler kind":"TargetGroupBinding","source":"kind source: /, Kind="}
{"level":"info","ts":1647955290.8508558,"logger":"controller-runtime.manager.controller.targetGroupBinding","msg":"Starting Controller","reconciler group":"elbv2.k8s.aws","reconciler kind":"TargetGroupBinding"}
{"level":"info","ts":1647955290.851058,"logger":"controller-runtime.manager.controller.ingress","msg":"Starting EventSource","source":"channel source: 0xc000291770"}
{"level":"info","ts":1647955290.8511033,"logger":"controller-runtime.manager.controller.ingress","msg":"Starting EventSource","source":"channel source: 0xc0002917c0"}
{"level":"info","ts":1647955290.8511171,"logger":"controller-runtime.manager.controller.ingress","msg":"Starting EventSource","source":"kind source: /, Kind="}
{"level":"info","ts":1647955290.8511248,"logger":"controller-runtime.manager.controller.ingress","msg":"Starting EventSource","source":"kind source: /, Kind="}
{"level":"info","ts":1647955290.8511307,"logger":"controller-runtime.manager.controller.ingress","msg":"Starting EventSource","source":"channel source: 0xc000291810"}
{"level":"info","ts":1647955290.8511422,"logger":"controller-runtime.manager.controller.ingress","msg":"Starting EventSource","source":"channel source: 0xc000291ae0"}
{"level":"info","ts":1647955290.851153,"logger":"controller-runtime.manager.controller.ingress","msg":"Starting EventSource","source":"kind source: /, Kind="}
{"level":"info","ts":1647955290.851159,"logger":"controller-runtime.manager.controller.ingress","msg":"Starting EventSource","source":"kind source: /, Kind="}
{"level":"info","ts":1647955290.8511693,"logger":"controller-runtime.manager.controller.ingress","msg":"Starting Controller"}
{"level":"info","ts":1647955290.8512616,"logger":"controller-runtime.manager.controller.service","msg":"Starting EventSource","source":"kind source: /, Kind="}
{"level":"info","ts":1647955290.851282,"logger":"controller-runtime.manager.controller.service","msg":"Starting Controller"}
{"level":"info","ts":1647955290.9514709,"logger":"controller-runtime.manager.controller.service","msg":"Starting workers","worker count":3}
{"level":"info","ts":1647955290.951765,"logger":"controller-runtime.manager.controller.targetGroupBinding","msg":"Starting workers","reconciler group":"elbv2.k8s.aws","reconciler kind":"TargetGroupBinding","worker count":3}
{"level":"info","ts":1647955290.9533036,"logger":"controller-runtime.manager.controller.ingress","msg":"Starting workers","worker count":3}
{"level":"info","ts":1647955619.9345033,"logger":"backend-sg-provider","msg":"creating securityGroup","name":"k8s-traffic-chatapp-a13366fdce"}
{"level":"info","ts":1647955620.1525552,"logger":"controllers.ingress","msg":"Auto Create SG","LB SGs":[{"$ref":"#/resources/AWS::EC2::SecurityGroup/ManagedLBSecurityGroup/status/groupID"},"sg-0c4b03c65cec21879"],"backend SG":"sg-0c4b03c65cec21879"}
{"level":"info","ts":1647955620.1579516,"logger":"controllers.ingress","msg":"successfully built model","model":"{\"id\":\"application/app-ingress\",\"resources\":{\"AWS::EC2::SecurityGroup\":{\"ManagedLBSecurityGroup\":{\"spec\":{\"groupName\":\"k8s-applicat-appingre-64f3a04718\",\"description\":\"[k8s] Managed SecurityGroup for LoadBalancer\",\"ingress\":[{\"ipProtocol\":\"tcp\",\"fromPort\":80,\"toPort\":80,\"ipRanges\":[{\"cidrIP\":\"0.0.0.0/0\"}]}]}}},\"AWS::ElasticLoadBalancingV2::Listener\":{\"80\":{\"spec\":{\"loadBalancerARN\":{\"$ref\":\"#/resources/AWS::ElasticLoadBalancingV2::LoadBalancer/LoadBalancer/status/loadBalancerARN\"},\"port\":80,\"protocol\":\"HTTP\",\"defaultActions\":[{\"type\":\"fixed-response\",\"fixedResponseConfig\":{\"contentType\":\"text/plain\",\"statusCode\":\"404\"}}]}}},\"AWS::ElasticLoadBalancingV2::ListenerRule\":{\"80:1\":{\"spec\":{\"listenerARN\":{\"$ref\":\"#/resources/AWS::ElasticLoadBalancingV2::Listener/80/status/listenerARN\"},\"priority\":1,\"actions\":[{\"type\":\"forward\",\"forwardConfig\":{\"targetGroups\":[{\"targetGroupARN\":{\"$ref\":\"#/resources/AWS::ElasticLoadBalancingV2::TargetGroup/application/app-ingress-backendapp:8000/status/targetGroupARN\"}}]}}],\"conditions\":[{\"field\":\"host-header\",\"hostHeaderConfig\":{\"values\":[\"devpriyachatapp.ml\"]}},{\"field\":\"path-pattern\",\"pathPatternConfig\":{\"values\":[\"/*\"]}}]}}},\"AWS::ElasticLoadBalancingV2::LoadBalancer\":{\"LoadBalancer\":{\"spec\":{\"name\":\"k8s-applicat-appingre-1fe2d299df\",\"type\":\"application\",\"scheme\":\"internet-facing\",\"ipAddressType\":\"ipv4\",\"subnetMapping\":[{\"subnetID\":\"subnet-00a182b7474ac6618\"},{\"subnetID\":\"subnet-059f7fb460c330dbd\"},{\"subnetID\":\"subnet-0a296f8521f5ae55c\"}],\"securityGroups\":[{\"$ref\":\"#/resources/AWS::EC2::SecurityGroup/ManagedLBSecurityGroup/status/groupID\"},\"sg-0c4b03c65cec21879\"]}}},\"AWS::ElasticLoadBalancingV2::TargetGroup\":{\"application/app-ingress-backendapp:8000\":{\"spec\":{\"name\":\"k8s-applicat-backenda-13671adb6d\",\"targetType\":\"instance\",\"port\":31679,\"protocol\":\"HTTP\",\"protocolVersion\":\"HTTP1\",\"ipAddressType\":\"ipv4\",\"healthCheckConfig\":{\"port\":\"traffic-port\",\"protocol\":\"HTTP\",\"path\":\"/\",\"matcher\":{\"httpCode\":\"200\"},\"intervalSeconds\":15,\"timeoutSeconds\":5,\"healthyThresholdCount\":2,\"unhealthyThresholdCount\":2}}}},\"K8S::ElasticLoadBalancingV2::TargetGroupBinding\":{\"application/app-ingress-backendapp:8000\":{\"spec\":{\"template\":{\"metadata\":{\"name\":\"k8s-applicat-backenda-13671adb6d\",\"namespace\":\"application\",\"creationTimestamp\":null},\"spec\":{\"targetGroupARN\":{\"$ref\":\"#/resources/AWS::ElasticLoadBalancingV2::TargetGroup/application/app-ingress-backendapp:8000/status/targetGroupARN\"},\"targetType\":\"instance\",\"serviceRef\":{\"name\":\"backendapp\",\"port\":8000},\"networking\":{\"ingress\":[{\"from\":[{\"securityGroup\":{\"groupID\":\"sg-0c4b03c65cec21879\"}}],\"ports\":[{\"protocol\":\"TCP\",\"port\":31679}]}]},\"ipAddressType\":\"ipv4\"}}}}}}}"}
{"level":"info","ts":1647955620.4296353,"logger":"controllers.ingress","msg":"creating securityGroup","resourceID":"ManagedLBSecurityGroup"}
{"level":"info","ts":1647955620.6296098,"logger":"controllers.ingress","msg":"created securityGroup","resourceID":"ManagedLBSecurityGroup","securityGroupID":"sg-0519a9de4d9d4bd6b"}
{"level":"info","ts":1647955620.7455797,"msg":"authorizing securityGroup ingress","securityGroupID":"sg-0519a9de4d9d4bd6b","permission":[{"FromPort":80,"IpProtocol":"tcp","IpRanges":[{"CidrIp":"0.0.0.0/0","Description":""}],"Ipv6Ranges":null,"PrefixListIds":null,"ToPort":80,"UserIdGroupPairs":null}]}
{"level":"info","ts":1647955620.9335005,"msg":"authorized securityGroup ingress","securityGroupID":"sg-0519a9de4d9d4bd6b"}
{"level":"info","ts":1647955620.9599864,"logger":"controllers.ingress","msg":"creating targetGroup","stackID":"application/app-ingress","resourceID":"application/app-ingress-backendapp:8000"}
{"level":"info","ts":1647955621.2154558,"logger":"controllers.ingress","msg":"created targetGroup","stackID":"application/app-ingress","resourceID":"application/app-ingress-backendapp:8000","arn":"arn:aws:elasticloadbalancing:us-east-2:285938675002:targetgroup/k8s-applicat-backenda-13671adb6d/571b1e2eaf169808"}
{"level":"info","ts":1647955621.255644,"logger":"controllers.ingress","msg":"creating loadBalancer","stackID":"application/app-ingress","resourceID":"LoadBalancer"}
{"level":"info","ts":1647955621.7232056,"logger":"controllers.ingress","msg":"created loadBalancer","stackID":"application/app-ingress","resourceID":"LoadBalancer","arn":"arn:aws:elasticloadbalancing:us-east-2:285938675002:loadbalancer/app/k8s-applicat-appingre-1fe2d299df/a15df3512d7012b9"}
{"level":"info","ts":1647955621.7582073,"logger":"controllers.ingress","msg":"creating listener","stackID":"application/app-ingress","resourceID":"80"}
{"level":"info","ts":1647955621.8115287,"logger":"controllers.ingress","msg":"created listener","stackID":"application/app-ingress","resourceID":"80","arn":"arn:aws:elasticloadbalancing:us-east-2:285938675002:listener/app/k8s-applicat-appingre-1fe2d299df/a15df3512d7012b9/263bff30159b9a49"}
{"level":"info","ts":1647955621.8458445,"logger":"controllers.ingress","msg":"creating listener rule","stackID":"application/app-ingress","resourceID":"80:1"}
{"level":"info","ts":1647955621.903345,"logger":"controllers.ingress","msg":"created listener rule","stackID":"application/app-ingress","resourceID":"80:1","arn":"arn:aws:elasticloadbalancing:us-east-2:285938675002:listener-rule/app/k8s-applicat-appingre-1fe2d299df/a15df3512d7012b9/263bff30159b9a49/4ca2fa1f7a158c18"}
{"level":"info","ts":1647955621.9034245,"logger":"controllers.ingress","msg":"creating targetGroupBinding","stackID":"application/app-ingress","resourceID":"application/app-ingress-backendapp:8000"}
{"level":"info","ts":1647955621.9652407,"logger":"controllers.ingress","msg":"created targetGroupBinding","stackID":"application/app-ingress","resourceID":"application/app-ingress-backendapp:8000","targetGroupBinding":{"namespace":"application","name":"k8s-applicat-backenda-13671adb6d"}}
{"level":"info","ts":1647955622.1003528,"logger":"controllers.ingress","msg":"successfully deployed model","ingressGroup":"application/app-ingress"}
{"level":"info","ts":1647955622.1588647,"msg":"authorizing securityGroup ingress","securityGroupID":"sg-07c14ac0b1866c916","permission":[{"FromPort":31679,"IpProtocol":"tcp","IpRanges":null,"Ipv6Ranges":null,"PrefixListIds":null,"ToPort":31679,"UserIdGroupPairs":[{"Description":"elbv2.k8s.aws/targetGroupBinding=shared","GroupId":"sg-0c4b03c65cec21879","GroupName":null,"PeeringStatus":null,"UserId":null,"VpcId":null,"VpcPeeringConnectionId":null}]}]}
{"level":"info","ts":1647955622.355755,"msg":"authorized securityGroup ingress","securityGroupID":"sg-07c14ac0b1866c916"}
{"level":"info","ts":1647955622.546617,"msg":"registering targets","arn":"arn:aws:elasticloadbalancing:us-east-2:285938675002:targetgroup/k8s-applicat-backenda-13671adb6d/571b1e2eaf169808","targets":[{"AvailabilityZone":null,"Id":"i-003f36c77c62e2fb7","Port":31679},{"AvailabilityZone":null,"Id":"i-0271ec2da0c20e8db","Port":31679}]}
{"level":"info","ts":1647955622.7074885,"msg":"registered targets","arn":"arn:aws:elasticloadbalancing:us-east-2:285938675002:targetgroup/k8s-applicat-backenda-13671adb6d/571b1e2eaf169808"}

C:\Users\User>kubectl get ingress -n application
NAME          CLASS   HOSTS                ADDRESS                                                                  PORTS   AGE
app-ingress   alb     devpriyachatapp.ml   k8s-applicat-appingre-1fe2d299df-469470152.us-east-2.elb.amazonaws.com   80      108s

C:\Users\User>



































